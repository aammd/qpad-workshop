---
title: "Extras"
author: "Peter Solymos <solymos@ualberta.ca>"
---

## Linear feature effects

## Censored Poisson data as detection/non-detection


$W$ is random variable based on $Y > 0$, that follows a Bernoulli distribution with probability $p$: $W=I_{(Y>0)}$, $W \mid p \sim Bernoulli(p)$, $W \in (0,1)$. 
The Bernoulli probability mass function is defined as:
$P(W=w) = p(1-p)=(1-e^{-\lambda} ) e^{-\lambda} = p^w (1-p)^{(1-w)}$.

Censored Poisson distribution with mean $\lambda$ is: $(Y \mid \lambda) \sim  Poisson(\lambda)$, $Y=1,2,3,\ldots$, $(Y^* \mid \lambda) ~ CensoredPoisson(\lambda)$, $Y^* \in (0,1)$.

The Censored Poisson probability mass function: $P(Y^*=y^*)=I_{(Y>0)} (1-e^{-\lambda} )+I_{(Y=0)} e^{-\lambda}$, $= w (1-e^{-\lambda})+(1-w) e^{-\lambda}$, $= (1-e^{-\lambda} )^w (e^{-\lambda})^{(1-w)}$, $= p^w (1-p)^{(1-w)}$.

Based on the equivalence, $p=1-e^{-\lambda}$, a Binomial GLM with the complementary log-log (cloglog) link function is equivalent to a Censored Poisson model with logarithmic link:
$1-p=e^{-\lambda}$, $log(1-p)={-\lambda}$, $-log(1-p)=e^\mu=\lambda$, $log(-log(1-p))=X^T\hat{\beta}=\mu=log(\lambda)$, where $X^T\hat{\beta}$ is the linear predictor.

```{r}
library(mefa4)             # data manipulation
load("data/josm-data.rda") # JOSM data

ytot <- Xtab(~ SiteID + SpeciesID, josm$counts[josm$counts$DetectType1 != "V",])
sort(colMeans(ytot))

#spp <- "OVEN"
spp <- "BOCH" # works better for species with lower mean counts

x <- data.frame(
  josm$surveys, 
  y=ytot[rownames(josm$surveys), spp])

x$y01 <- ifelse(x$y > 0, 1, 0)
table(x$y, x$y01)

mP <- glm(y ~ Decid * ConifWet, data=x, family=poisson)
mB <- glm(y01 ~ Decid * ConifWet, data=x, family=binomial("cloglog"))
cbind(Poisson=coef(mP), cloglog=coef(mB))

mean(fitted(mP))
mean(fitted(mB))
mean(exp(model.matrix(mB) %*% coef(mB)))
```


## Ideas from Juan

1) Package gratia has very nice gam evaluation and visualization tools
```
devtools::install_github('gavinsimpson/gratia') 
appraise(model_object) # residuals and fit
draw(model_object) # marginal effects (response curves)
```

2) Package randomForest allows for a quick and (more) intuitive insight for exploration of variable importance; it is a bit hard to see from the regression tree embebbed in the code

```
# the model formula
model <- count ~ cov1 + cov 2 + ... + covN
# fit the RF model
model_fit  <- randomForest(model, data=data, ntree = 1000, importance=T)
importance(model_fit) # variable importance summary 
plot(model_fit) # nice var importance plot
```


